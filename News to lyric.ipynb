{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News to poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pprint import pprint\n",
    "from nltk.parse.stanford import StanfordDependencyParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('British', 'JJ'), ('scientist', 'NN'), ('says', 'VBZ'), ('memories', 'NNS'), ('of', 'IN'), ('spending', 'VBG'), ('the', 'DT'), ('night', 'NN'), ('with', 'IN'), ('Marilyn', 'NNP'), ('Monroe', 'NNP'), ('could', 'MD'), ('be', 'VB'), ('implanted', 'VBN'), ('into', 'IN'), ('the', 'DT'), ('brain', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "partial_grammar = \"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "\"\"\"\n",
    "title = \"British scientist says memories of spending the night with Marilyn Monroe could be implanted into the brain\"\n",
    "title_words = nltk.word_tokenize(title)\n",
    "title_pos = nltk.pos_tag(title_words)\n",
    "print title_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lex_grammar(words_pos):\n",
    "    lexicon = {}\n",
    "    for word, pos in sorted(words_pos, key=lambda (a, b): b):\n",
    "        if pos not in lexicon:\n",
    "            lexicon[pos] = []\n",
    "        lexicon[pos].append(word)\n",
    "    return \"\\n\".join(\"{pos} -> {words}\".format(pos=pos, words=\"|\".join(ws)) for pos, ws in lexicon.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "second_part_of_grammar = lex_grammar(title_pos)\n",
    "grammar = partial_grammar + second_part_of_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S -> NP VP\n",
      "PP -> P NP\n",
      "NP -> Det N | Det N PP | 'I'\n",
      "VP -> V NP | VP PP\n",
      "MD -> could\n",
      "VB -> be\n",
      "VBG -> spending\n",
      "NN -> scientist|night|brain\n",
      "VBN -> implanted\n",
      "JJ -> British\n",
      "IN -> of|with|into\n",
      "VBZ -> says\n",
      "DT -> the|the\n",
      "NNS -> memories\n",
      "NNP -> Marilyn|Monroe\n"
     ]
    }
   ],
   "source": [
    "print grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding you should download zip-archive with stanford parser:\n",
    "\n",
    "http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip\n",
    "\n",
    "We need two files from there:\n",
    "- stanford/stanford-parser.jar\n",
    "- stanford/stanford-parser-3.5.2-models.jar\n",
    "\n",
    "Create a directory 'stanford' and put them there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British scientist says memories of spending the night with Marilyn Monroe could be implanted into the brain\n",
      "[((u'says', u'VBZ'), u'nsubj', (u'scientist', u'NN')),\n",
      " ((u'scientist', u'NN'), u'amod', (u'British', u'JJ')),\n",
      " ((u'says', u'VBZ'), u'dobj', (u'memories', u'NNS')),\n",
      " ((u'memories', u'NNS'), u'nmod', (u'spending', u'NN')),\n",
      " ((u'spending', u'NN'), u'case', (u'of', u'IN')),\n",
      " ((u'says', u'VBZ'), u'nmod:tmod', (u'night', u'NN')),\n",
      " ((u'night', u'NN'), u'det', (u'the', u'DT')),\n",
      " ((u'says', u'VBZ'), u'advcl', (u'implanted', u'VBN')),\n",
      " ((u'implanted', u'VBN'), u'mark', (u'with', u'IN')),\n",
      " ((u'implanted', u'VBN'), u'nsubjpass', (u'Monroe', u'NNP')),\n",
      " ((u'Monroe', u'NNP'), u'compound', (u'Marilyn', u'NNP')),\n",
      " ((u'implanted', u'VBN'), u'aux', (u'could', u'MD')),\n",
      " ((u'implanted', u'VBN'), u'auxpass', (u'be', u'VB')),\n",
      " ((u'implanted', u'VBN'), u'nmod', (u'brain', u'NN')),\n",
      " ((u'brain', u'NN'), u'case', (u'into', u'IN')),\n",
      " ((u'brain', u'NN'), u'det', (u'the', u'DT'))]\n"
     ]
    }
   ],
   "source": [
    "parser = StanfordDependencyParser(path_to_jar='stanford/stanford-parser.jar',\\\n",
    "                                  path_to_models_jar='stanford/stanford-parser-3.5.2-models.jar')\n",
    "parsed_tree = list(parser.parse(title_words))\n",
    "\n",
    "print title\n",
    "pprint(list(parsed_tree[0].triples()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'put', u'VBN'), u'nsubjpass', (u'ball', u'NN')),\n",
      " ((u'ball', u'NN'), u'det', (u'A', u'DT')),\n",
      " ((u'put', u'VBN'), u'auxpass', (u'is', u'VBZ')),\n",
      " ((u'put', u'VBN'), u'nmod', (u'box', u'NN')),\n",
      " ((u'box', u'NN'), u'case', (u'into', u'IN')),\n",
      " ((u'box', u'NN'), u'det', (u'the', u'DT'))]\n"
     ]
    }
   ],
   "source": [
    "test_sent = \"A ball is put into the box\"\n",
    "pprint(list(list(parser.parse(test_sent.split(' ')))[0].triples()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Morphological analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def tokenize(s, pattern=r'\\W+'):\n",
    "    return filter(None, re.split(pattern, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict\n",
    "from itertools import izip_longest, chain\n",
    "d = cmudict.dict()\n",
    "\n",
    "poem_file = open('data/guinea_pig.txt')\n",
    "corpus = poem_file.readlines()\n",
    "poem_file.close()\n",
    "\n",
    "poem_structure_1 = [\n",
    "    (8, [1,3,5,7],),\n",
    "    (8, [1,3,5,7],),\n",
    "    (8, [1,3,5,7],),\n",
    "    (8, [1,3,5,7],),\n",
    "]\n",
    "# for line in corpus:\n",
    "#     for token in tokenize(line):\n",
    "#         print token, d.get(token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'DH', u'EH1', u'R'], [u'W', u'AA1', u'Z'], [u'AH0'], [u'L', u'IH1', u'T', u'AH0', u'L'], [u'G', u'IH1', u'N', u'IY0'], [u'P', u'IH1', u'G']] [True, True, False, True, False, True, False, True]\n",
      "[[u'DH', u'EH1', u'R'], [u'W', u'AA1', u'Z'], [u'EY1'], [u'L', u'IH1', u'T', u'AH0', u'L'], [u'G', u'IH1', u'N', u'IY0'], [u'P', u'IH1', u'G']] [True, True, True, True, False, True, False, True]\n",
      "[[u'DH', u'EH1', u'R'], [u'W', u'AH1', u'Z'], [u'AH0'], [u'L', u'IH1', u'T', u'AH0', u'L'], [u'G', u'IH1', u'N', u'IY0'], [u'P', u'IH1', u'G']] [True, True, False, True, False, True, False, True]\n",
      "[[u'DH', u'EH1', u'R'], [u'W', u'AH1', u'Z'], [u'EY1'], [u'L', u'IH1', u'T', u'AH0', u'L'], [u'G', u'IH1', u'N', u'IY0'], [u'P', u'IH1', u'G']] [True, True, True, True, False, True, False, True]\n",
      "[[u'DH', u'EH1', u'R'], [u'W', u'AH0', u'Z'], [u'AH0'], [u'L', u'IH1', u'T', u'AH0', u'L'], [u'G', u'IH1', u'N', u'IY0'], [u'P', u'IH1', u'G']] [True, False, False, True, False, True, False, True]\n",
      "[[u'DH', u'EH1', u'R'], [u'W', u'AH0', u'Z'], [u'EY1'], [u'L', u'IH1', u'T', u'AH0', u'L'], [u'G', u'IH1', u'N', u'IY0'], [u'P', u'IH1', u'G']] [True, False, True, True, False, True, False, True]\n",
      "[[u'DH', u'EH1', u'R'], [u'W', u'AO1', u'Z'], [u'AH0'], [u'L', u'IH1', u'T', u'AH0', u'L'], [u'G', u'IH1', u'N', u'IY0'], [u'P', u'IH1', u'G']] [True, True, False, True, False, True, False, True]\n",
      "[[u'DH', u'EH1', u'R'], [u'W', u'AO1', u'Z'], [u'EY1'], [u'L', u'IH1', u'T', u'AH0', u'L'], [u'G', u'IH1', u'N', u'IY0'], [u'P', u'IH1', u'G']] [True, True, True, True, False, True, False, True]\n",
      "\n",
      "[[u'HH', u'IY1'], [u'AO1', u'L', u'W', u'EY2', u'Z'], [u'W', u'AO1', u'K', u'T'], [u'AH0', u'P', u'AA1', u'N'], [u'HH', u'IH1', u'Z'], [u'F', u'IY1', u'T']] [True, True, False, True, False, True, True, True]\n",
      "[[u'HH', u'IY1'], [u'AO1', u'L', u'W', u'EY2', u'Z'], [u'W', u'AO1', u'K', u'T'], [u'AH0', u'P', u'AA1', u'N'], [u'HH', u'IH0', u'Z'], [u'F', u'IY1', u'T']] [True, True, False, True, False, True, False, True]\n",
      "[[u'HH', u'IY1'], [u'AO1', u'L', u'W', u'IY0', u'Z'], [u'W', u'AO1', u'K', u'T'], [u'AH0', u'P', u'AA1', u'N'], [u'HH', u'IH1', u'Z'], [u'F', u'IY1', u'T']] [True, True, False, True, False, True, True, True]\n",
      "[[u'HH', u'IY1'], [u'AO1', u'L', u'W', u'IY0', u'Z'], [u'W', u'AO1', u'K', u'T'], [u'AH0', u'P', u'AA1', u'N'], [u'HH', u'IH0', u'Z'], [u'F', u'IY1', u'T']] [True, True, False, True, False, True, False, True]\n",
      "\n",
      "[[u'AH0', u'N', u'D'], [u'W', u'AY1', u'L'], [u'HH', u'IY1'], [u'R', u'AH1', u'N'], [u'AE1', u'Z'], [u'AY1'], [u'AE1', u'M'], [u'T', u'OW1', u'L', u'D']] [False, True, True, True, True, True, True, True]\n",
      "[[u'AH0', u'N', u'D'], [u'W', u'AY1', u'L'], [u'HH', u'IY1'], [u'R', u'AH1', u'N'], [u'AE1', u'Z'], [u'AY1'], [u'EY1', u'EH1', u'M'], [u'T', u'OW1', u'L', u'D']] [False, True, True, True, True, True, True, True, True]\n",
      "[[u'AH0', u'N', u'D'], [u'W', u'AY1', u'L'], [u'HH', u'IY1'], [u'R', u'AH1', u'N'], [u'EH1', u'Z'], [u'AY1'], [u'AE1', u'M'], [u'T', u'OW1', u'L', u'D']] [False, True, True, True, True, True, True, True]\n",
      "[[u'AH0', u'N', u'D'], [u'W', u'AY1', u'L'], [u'HH', u'IY1'], [u'R', u'AH1', u'N'], [u'EH1', u'Z'], [u'AY1'], [u'EY1', u'EH1', u'M'], [u'T', u'OW1', u'L', u'D']] [False, True, True, True, True, True, True, True, True]\n",
      "[[u'AH0', u'N', u'D'], [u'HH', u'W', u'AY1', u'L'], [u'HH', u'IY1'], [u'R', u'AH1', u'N'], [u'AE1', u'Z'], [u'AY1'], [u'AE1', u'M'], [u'T', u'OW1', u'L', u'D']] [False, True, True, True, True, True, True, True]\n",
      "[[u'AH0', u'N', u'D'], [u'HH', u'W', u'AY1', u'L'], [u'HH', u'IY1'], [u'R', u'AH1', u'N'], [u'AE1', u'Z'], [u'AY1'], [u'EY1', u'EH1', u'M'], [u'T', u'OW1', u'L', u'D']] [False, True, True, True, True, True, True, True, True]\n",
      "[[u'AH0', u'N', u'D'], [u'HH', u'W', u'AY1', u'L'], [u'HH', u'IY1'], [u'R', u'AH1', u'N'], [u'EH1', u'Z'], [u'AY1'], [u'AE1', u'M'], [u'T', u'OW1', u'L', u'D']] [False, True, True, True, True, True, True, True]\n",
      "[[u'AH0', u'N', u'D'], [u'HH', u'W', u'AY1', u'L'], [u'HH', u'IY1'], [u'R', u'AH1', u'N'], [u'EH1', u'Z'], [u'AY1'], [u'EY1', u'EH1', u'M'], [u'T', u'OW1', u'L', u'D']] [False, True, True, True, True, True, True, True, True]\n",
      "[[u'AE1', u'N', u'D'], [u'W', u'AY1', u'L'], [u'HH', u'IY1'], [u'R', u'AH1', u'N'], [u'AE1', u'Z'], [u'AY1'], [u'AE1', u'M'], [u'T', u'OW1', u'L', u'D']] [True, True, True, True, True, True, True, True]\n",
      "[[u'AE1', u'N', u'D'], [u'W', u'AY1', u'L'], [u'HH', u'IY1'], [u'R', u'AH1', u'N'], [u'AE1', u'Z'], [u'AY1'], [u'EY1', u'EH1', u'M'], [u'T', u'OW1', u'L', u'D']] [True, True, True, True, True, True, True, True, True]\n",
      "[[u'AE1', u'N', u'D'], [u'W', u'AY1', u'L'], [u'HH', u'IY1'], [u'R', u'AH1', u'N'], [u'EH1', u'Z'], [u'AY1'], [u'AE1', u'M'], [u'T', u'OW1', u'L', u'D']] [True, True, True, True, True, True, True, True]\n",
      "[[u'AE1', u'N', u'D'], [u'W', u'AY1', u'L'], [u'HH', u'IY1'], [u'R', u'AH1', u'N'], [u'EH1', u'Z'], [u'AY1'], [u'EY1', u'EH1', u'M'], [u'T', u'OW1', u'L', u'D']] [True, True, True, True, True, True, True, True, True]\n",
      "[[u'AE1', u'N', u'D'], [u'HH', u'W', u'AY1', u'L'], [u'HH', u'IY1'], [u'R', u'AH1', u'N'], [u'AE1', u'Z'], [u'AY1'], [u'AE1', u'M'], [u'T', u'OW1', u'L', u'D']] [True, True, True, True, True, True, True, True]\n",
      "[[u'AE1', u'N', u'D'], [u'HH', u'W', u'AY1', u'L'], [u'HH', u'IY1'], [u'R', u'AH1', u'N'], [u'AE1', u'Z'], [u'AY1'], [u'EY1', u'EH1', u'M'], [u'T', u'OW1', u'L', u'D']] [True, True, True, True, True, True, True, True, True]\n",
      "[[u'AE1', u'N', u'D'], [u'HH', u'W', u'AY1', u'L'], [u'HH', u'IY1'], [u'R', u'AH1', u'N'], [u'EH1', u'Z'], [u'AY1'], [u'AE1', u'M'], [u'T', u'OW1', u'L', u'D']] [True, True, True, True, True, True, True, True]\n",
      "[[u'AE1', u'N', u'D'], [u'HH', u'W', u'AY1', u'L'], [u'HH', u'IY1'], [u'R', u'AH1', u'N'], [u'EH1', u'Z'], [u'AY1'], [u'EY1', u'EH1', u'M'], [u'T', u'OW1', u'L', u'D']] [True, True, True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "def num_of_syllables(phonetic_translation):\n",
    "    number_of_syllables = 0\n",
    "    for el in phonetic_translation:\n",
    "        if el[-1].isdigit() == True:\n",
    "            number_of_syllables += 1\n",
    "        else:\n",
    "            pass\n",
    "    return number_of_syllables\n",
    "        \n",
    "def is_stressed(phoneme):\n",
    "    if phoneme[-1] == '1':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def stress_mask(phonemes):\n",
    "    return [is_stressed(ph) for ph in phonemes if is_vowel(ph)]\n",
    "\n",
    "def combinations(*args):\n",
    "    if len(args) == 1:\n",
    "        return [[x] for x in args[0]]\n",
    "    combs = []\n",
    "    for item in args[0]:\n",
    "        for x in combinations(*args[1:]):\n",
    "            combs.append([item] + x)\n",
    "    return combs\n",
    "\n",
    "\n",
    "# helper function of validate_couplet(x,y)\n",
    "def validate_line(line, structure):\n",
    "    line_phonemes = [] \n",
    "    words = line.split(' ')\n",
    "    for word in words:\n",
    "        word_phonemes = d.get(word)\n",
    "        line_phonemes.append(word_phonemes)\n",
    "    for candidate in combinations(*line_phonemes):\n",
    "        stresses = stress_mask(chain.from_iterable(candidate))\n",
    "        print candidate, stresses\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "#returns true or false; does a couplet answer to the above mentioned scheme or not?\n",
    "def validate_couplet(couplet, structure):\n",
    "    pass\n",
    "\n",
    "def is_vowel(sound):\n",
    "    return sound[-1].isdigit()\n",
    "\n",
    "def is_rhyme(pron1, pron2):\n",
    "    vowels = [(i, sound) for i, sound in enumerate(pron1) if is_vowel(sound)]\n",
    "    if vowels:\n",
    "        idx, last_vowel = vowels[-1]\n",
    "        return pron1[-(idx + 1):] == pron2[-(idx + 1):]\n",
    "    else:\n",
    "        return False\n",
    "# #test\n",
    "# for pron1 in d.get('was'):\n",
    "#     for pron2 in d.get('because'):\n",
    "#         print pron1, pron2, is_rhyme(pron1, pron2)\n",
    "combinations([1,2,3],[4],[5,6],[7,8,9])\n",
    "validate_line(\"there was a little guinea pig\", [1,3,5,7])\n",
    "print\n",
    "validate_line(\"he always walked upon his feet\", None)\n",
    "print\n",
    "validate_line(\"and while he run as i am told\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.get(\"there was\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
